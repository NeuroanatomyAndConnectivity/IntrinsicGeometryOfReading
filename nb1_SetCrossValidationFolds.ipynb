{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3261d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import CCAtools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c24d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2PermCCA = \"PermCCA/\"  ### https://github.com/andersonwinkler/PermCCA\n",
    "### note environment requirement being phased out ^^\n",
    "palmPath = \"palm-alpha119_2/\"  ### path to palm\n",
    "### see https://github.com/andersonwinkler/PALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine as mlab_eng\n",
    "import matlab\n",
    "\n",
    "eng = mlab_eng.start_matlab()\n",
    "eng.addpath(path2PermCCA)\n",
    "eng.addpath(palmPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a2a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CCAtools.preprocessing import (\n",
    "    prep_confounds,\n",
    "    cube_root,\n",
    "    gauss_SM,\n",
    "    zscore,\n",
    "    normal_eqn_python,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f504f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set paths\n",
    "restricted_hcp_path = \"\"  ## set path to your copy of hcp restricted data\n",
    "hcp_behavior_data = \"\"  ### set path to your copy of hcp behvaioral data\n",
    "data_dir = \"\"  ### set to directory for data used in/throughout analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f9b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load behavioral data\n",
    "### this is HCP restricted data. for access see\n",
    "### https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage\n",
    "RestrictedData = pd.read_csv(restricted_hcp_path, index_col=\"Subject\")\n",
    "### load non-restricted hcp behavioral data\n",
    "BehData = pd.read_csv(hcp_behavior_data, index_col=\"Subject\")\n",
    "### merge the dtaframes\n",
    "fullData = BehData.merge(RestrictedData, on=\"Subject\")\n",
    "fullData.index = fullData.index.map(str)\n",
    "fullData.index = fullData.index.map(str)\n",
    "### load in square_root of total hemisphere surface area for each subject\n",
    "Larea = pd.read_csv(f\"{data_dir}LareaFactors.csv\")\n",
    "Larea.rename(index={0: \"Larea\"}, inplace=True)\n",
    "Rarea = pd.read_csv(f\"{data_dir}RareaFactors.csv\")\n",
    "Rarea.rename(index={0: \"Rarea\"}, inplace=True)\n",
    "area = pd.concat([Larea, Rarea]).T\n",
    "area.index.names = [\"Subject\"]\n",
    "fullData = area.join(fullData, on=\"Subject\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ac324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### remove subjects which throw off permuations in later steps.\n",
    "### see: doi.org/10.1016/j.neuroimage.2015.05.092\n",
    "fullData.drop([\"168240\", \"376247\"], inplace=True)\n",
    "Larea.T.drop([\"168240\", \"376247\"], inplace=True)\n",
    "Rarea.T.drop([\"168240\", \"376247\"], inplace=True)\n",
    "area.drop([\"168240\", \"376247\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77bf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_categories = fullData[\n",
    "    [\n",
    "        \"Acquisition\",\n",
    "        \"Gender\",\n",
    "        \"Age_in_Yrs\",\n",
    "        \"Height\",\n",
    "        \"Weight\",\n",
    "        \"BPSystolic\",\n",
    "        \"BPDiastolic\",\n",
    "        \"FS_IntraCranial_Vol\",\n",
    "        \"FS_BrainSeg_Vol\",\n",
    "        \"Larea\",\n",
    "        \"Rarea\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "Tasks = [\n",
    "    \"CardSort_Unadj\",\n",
    "    \"DDisc_AUC_200\",\n",
    "    \"DDisc_AUC_40K\",\n",
    "    \"Dexterity_Unadj\",\n",
    "    \"ER40ANG\",\n",
    "    \"ER40FEAR\",\n",
    "    \"ER40NOE\",\n",
    "    \"ER40SAD\",\n",
    "    \"ER40_CR\",\n",
    "    \"Flanker_Unadj\",\n",
    "    \"IWRD_TOT\",\n",
    "    \"ListSort_Unadj\",\n",
    "    \"MMSE_Score\",\n",
    "    \"Mars_Final\",\n",
    "    \"Odor_Unadj\",\n",
    "    \"PMAT24_A_CR\",\n",
    "    \"PicSeq_Unadj\",\n",
    "    \"PicVocab_Unadj\",\n",
    "    \"ProcSpeed_Unadj\",\n",
    "    \"ReadEng_Unadj\",\n",
    "    \"SCPT_SEN\",\n",
    "    \"SCPT_SPEC\",\n",
    "    \"Taste_Unadj\",\n",
    "    \"VSPLOT_CRTE\",\n",
    "    \"Noise_Comp\",\n",
    "    \"EVA_Denom\",\n",
    "]\n",
    "\n",
    "### note we drop subjects with nans in any of the above tasks ^^\n",
    "### this was to ensure we only used well sampled subjects on all facets and could use them for later studies\n",
    "\n",
    "TaskOfInterest = [\"ReadEng_Unadj\"]\n",
    "subjSM = fullData[Tasks]\n",
    "\n",
    "\n",
    "full_subjList = conf_categories.merge(subjSM, on=\"Subject\").dropna().index\n",
    "full_subjList.to_series().to_csv(\"SubjectInclusionList.csv\")\n",
    "\n",
    "conf_categories = conf_categories.loc[full_subjList]\n",
    "subjSM = subjSM.loc[full_subjList]\n",
    "\n",
    "complete_data = fullData.loc[full_subjList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47767dac",
   "metadata": {},
   "source": [
    "### set up the cross validation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c854fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_groups(groups, groupedData):\n",
    "    \"\"\"\n",
    "    This function creates cross-validation groups from a list of lists, returning a dictionary\n",
    "    where each key is a fold number and each value is another dictionary with keys for the training\n",
    "    and test sets for that fold.\n",
    "\n",
    "    :param groups: A list of lists, where each sub-list contains subjects.\n",
    "    :return: A dictionary with fold numbers as keys and dictionaries of training and test sets as values.\n",
    "    \"\"\"\n",
    "    cv_folds = {}\n",
    "    for i in range(len(groups)):\n",
    "        test_set = groups[i]\n",
    "        test_set = pd.concat([groupedData.get_group(fam) for fam in test_set]).index\n",
    "        training_set = [\n",
    "            subject for j, group in enumerate(groups) if j != i for subject in group\n",
    "        ]\n",
    "        training_set = pd.concat(\n",
    "            [groupedData.get_group(fam) for fam in training_set]\n",
    "        ).index\n",
    "        fold_key = f\"fold{i + 1}\"\n",
    "        cv_folds[fold_key] = {\"training\": list(training_set), \"testing\": list(test_set)}\n",
    "    return cv_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "904d860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [0, 42, 19, 123, 10, 69, 33, 1, 1234, 9245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf4ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first iteration use data as is\n"
     ]
    }
   ],
   "source": [
    "### create 10 x 10K fold cross validation i.e. 100 unique training and testing folds\n",
    "nfolds = 10\n",
    "DataGroups = []\n",
    "FamilyGroups = []\n",
    "for seed in random_seeds:\n",
    "    ### create cross validation respecting family groups\n",
    "    ### in this case we'll do 10 fold cross-validation 10 times\n",
    "    np.random.seed(seed)\n",
    "    if seed == 0:\n",
    "        print(\"first iteration use data as is\")\n",
    "        fam_ids = np.unique(complete_data[\"Family_ID\"])\n",
    "        family_groups = np.array_split(fam_ids, nfolds)\n",
    "        ### group subjects by family ID\n",
    "        grouped_data = complete_data.groupby(\"Family_ID\")\n",
    "        DataGroups.append(grouped_data)\n",
    "        FamilyGroups.append(family_groups)\n",
    "    else:\n",
    "        np.random.shuffle(fam_ids)\n",
    "        family_groups = np.array_split(fam_ids, nfolds)\n",
    "        ### group subjects by family ID\n",
    "        grouped_data = complete_data.groupby(\"Family_ID\")\n",
    "        DataGroups.append(grouped_data)\n",
    "        FamilyGroups.append(family_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c26450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_Sets = {}\n",
    "x = 1\n",
    "for i, j in zip(FamilyGroups, DataGroups):\n",
    "    folds = create_cross_validation_groups(i, j)\n",
    "    fold_Sets[f\"fold set {x}\"] = folds\n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc1713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a789f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all folds into single dictionary\n",
    "all_folds = {}\n",
    "# Keep track of the fold number\n",
    "fold_number = 1\n",
    "# Loop through each fold set\n",
    "for fold_set_name, folds in fold_Sets.items():\n",
    "    # Loop through each sub-fold within the fold set\n",
    "    for fold_name, fold_data in folds.items():\n",
    "        # Construct a unique name for the fold and add it to the combined folds dictionary\n",
    "        all_folds[f\"fold{fold_number}\"] = fold_data\n",
    "        # Increment the fold number for the next iteration\n",
    "        fold_number += 1\n",
    "\n",
    "# Now, combined_folds is a single dictionary with 100 entries\n",
    "# Specify the filename you want to save the JSON data to\n",
    "filename = \"cross_validation_folds.json\"\n",
    "# Write the JSON data to a file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(all_folds, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b1713",
   "metadata": {},
   "source": [
    "# Finished \n",
    "Continue to notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56849a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
