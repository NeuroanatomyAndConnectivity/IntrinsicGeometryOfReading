{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3261d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import CCAtools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1a7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2PermCCA = \"PermCCA/\"  ### https://github.com/andersonwinkler/PermCCA\n",
    "### note environment requirement being phased out ^^\n",
    "palmPath = \"palm-alpha119_2/\"  ### path to palm\n",
    "### see https://github.com/andersonwinkler/PALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine as mlab_eng\n",
    "import matlab\n",
    "\n",
    "eng = mlab_eng.start_matlab()\n",
    "eng.addpath(path2PermCCA)\n",
    "eng.addpath(palmPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a2a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CCAtools.preprocessing import (\n",
    "    prep_confounds,\n",
    "    cube_root,\n",
    "    gauss_SM,\n",
    "    zscore,\n",
    "    normal_eqn_python,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a25854",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set paths\n",
    "restricted_hcp_path = \"\"  ## set path to your copy of hcp restricted data\n",
    "hcp_behavior_data = \"\"  ### set path to your copy of hcp behvaioral data\n",
    "data_dir = \"\"  ### set to directory for data used in/throughout analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f9b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load behavioral data\n",
    "### this is HCP restricted data. for access see\n",
    "### https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage\n",
    "RestrictedData = pd.read_csv(restricted_hcp_path, index_col=\"Subject\")\n",
    "### load non-restricted hcp behavioral data\n",
    "BehData = pd.read_csv(hcp_behavior_data, index_col=\"Subject\")\n",
    "### merge the dtaframes\n",
    "fullData = BehData.merge(RestrictedData, on=\"Subject\")\n",
    "fullData.index = fullData.index.map(str)\n",
    "fullData.index = fullData.index.map(str)\n",
    "### load in square_root of total hemisphere surface area for each subject\n",
    "Larea = pd.read_csv(f\"{data_dir}LareaFactors.csv\")\n",
    "Larea.rename(index={0: \"Larea\"}, inplace=True)\n",
    "Rarea = pd.read_csv(f\"{data_dir}RareaFactors.csv\")\n",
    "Rarea.rename(index={0: \"Rarea\"}, inplace=True)\n",
    "area = pd.concat([Larea, Rarea]).T\n",
    "area.index.names = [\"Subject\"]\n",
    "fullData = area.join(fullData, on=\"Subject\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5551bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Distances\n",
    "LSchaeferDist = pd.read_csv(f\"{data_dir}/Schaefer400Geodesic.L.mat.csv\")\n",
    "LSchaeferDist.rename(columns={\"Unnamed: 0\": \"Subject\"}, inplace=True)\n",
    "LSchaeferDist.set_index(\"Subject\", inplace=True)\n",
    "\n",
    "RSchaeferDist = pd.read_csv(f\"{data_dir}/Schaefer400Geodesic.R.mat.csv\")\n",
    "RSchaeferDist.rename(columns={\"Unnamed: 0\": \"Subject\"}, inplace=True)\n",
    "RSchaeferDist.set_index(\"Subject\", inplace=True)\n",
    "SchaeferDist = pd.concat([LSchaeferDist, RSchaeferDist], axis=1)\n",
    "SchaeferDist.index = SchaeferDist.index.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ac324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### remove a few subjects that don't have all the data to generate the right permutations\n",
    "SchaeferDist.drop([\"168240\", \"376247\"], inplace=True)\n",
    "fullData.drop([\"168240\", \"376247\"], inplace=True)\n",
    "Larea.T.drop([\"168240\", \"376247\"], inplace=True)\n",
    "Rarea.T.drop([\"168240\", \"376247\"], inplace=True)\n",
    "area.drop([\"168240\", \"376247\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb0059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### preprocessing functions we'll add to a package later\n",
    "from CCAtools.preprocessing import zscore\n",
    "\n",
    "\n",
    "def prep_confounds_local(confs, eng):\n",
    "    \"\"\"set the confounds up with gaussianization and normalization as done by smith et al 2015.\"\"\"\n",
    "    assert (\"palm\" in eng.path()) == True, \"add PermCCA to your matlab path\"\n",
    "    mat_data = matlab.double(\n",
    "        confs.values.tolist()\n",
    "    )  ### they actually included the binary acquisition data in the gaussianization\n",
    "    print(\"gaussianizing\")\n",
    "    gaussed = np.asarray(eng.palm_inormal(mat_data))\n",
    "    squared = gaussed[:, 1:] ** 2\n",
    "    ready_confs = np.hstack([gaussed, squared])\n",
    "    ready_confs = zscore(np.hstack([gaussed, squared]), ax=0)\n",
    "\n",
    "    return ready_confs\n",
    "\n",
    "\n",
    "def set_confounds(data, mlab_eng):\n",
    "    \"\"\"takes in a full data set of all HCP variables and extracts and preprocesses confounds to be regressed\"\"\"\n",
    "    eng = mlab_eng\n",
    "    confounds = data\n",
    "    gend = LabelEncoder().fit_transform(confounds[\"Gender\"])\n",
    "    acq = LabelEncoder().fit_transform(confounds[\"Acquisition\"])\n",
    "    acq[acq < 2] = 0\n",
    "    acq[acq > 0] = 1\n",
    "    df = confounds.copy()\n",
    "    df[\"Acquisition\"] = acq\n",
    "    df[\"Gender\"] = gend\n",
    "    df[\"FS_IntraCranial_Vol\"] = data[\"FS_IntraCranial_Vol\"].map(cube_root)\n",
    "    df[\"FS_BrainSeg_Vol\"] = data[\"FS_BrainSeg_Vol\"].map(cube_root)\n",
    "    df[\"Larea\"] = data[\"Larea\"].map(np.sqrt)\n",
    "    df[\"Rarea\"] = data[\"Rarea\"].map(np.sqrt)\n",
    "    confounds = prep_confounds_local(df, eng)\n",
    "    return confounds\n",
    "\n",
    "\n",
    "def preprocess_SM(data, confs, mlab_eng):\n",
    "    \"\"\"preprocess the subject measures. Guassianize and remove confounds.\"\"\"\n",
    "    eng = mlab_eng\n",
    "    assert (\"palm\" in eng.path()) == True, \"add PermCCA to your matlab path\"\n",
    "    data = data\n",
    "    gaussed = gauss_SM(data, eng)\n",
    "    residuals = normal_eqn_python(confs, gaussed)\n",
    "    cleaned = zscore(residuals)\n",
    "    cleaned = pd.DataFrame(cleaned, index=data.index, columns=data.columns)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def preprocessDists(data, confounds):\n",
    "    NET = data.copy()\n",
    "    dims = NET.shape\n",
    "    ##### check for vertices with no variance i.e guaranteed masks\n",
    "    steady_masks = np.where(np.sum(NET) == 0)[0]\n",
    "    valididx = np.where(np.sum(NET) != 0)[0]\n",
    "\n",
    "    if len(steady_masks) != 0:\n",
    "        NET = NET.iloc[:, valididx]\n",
    "\n",
    "    #     amNET = np.abs(np.nanmean(NET, axis=0))\n",
    "    NET1 = NET  # /amNET\n",
    "    NET1 = NET1 - np.mean(NET1, axis=0)\n",
    "    NET1 = NET1 / np.nanstd(NET1.values.flatten())\n",
    "    NET1 = normal_eqn_python(confounds, NET1)\n",
    "    NET1 = pd.DataFrame(NET1, columns=NET.columns, index=data.index)\n",
    "\n",
    "    if len(steady_masks) != 0:\n",
    "        out = np.zeros(dims)\n",
    "        out[:, valididx] = NET1.values\n",
    "        NET1 = pd.DataFrame(out, index=NET.index)\n",
    "\n",
    "    return NET1\n",
    "\n",
    "\n",
    "def clean_data(subjectList, all_confs, all_SM, all_dist, mlab=eng):\n",
    "    ### remove confounds for a group of subjects\n",
    "    ### always done independently to avoid leakage\n",
    "\n",
    "    ## set the confounds\n",
    "    confs = all_confs.loc[subjectList]\n",
    "    confs = set_confounds(confs, mlab)\n",
    "    ## regress them from behavioral measures\n",
    "    behavior = all_SM.loc[subjectList]\n",
    "    behavior = preprocess_SM(behavior, confs, mlab)\n",
    "    ## regress them from distance measures\n",
    "    distance = all_dist.loc[subjectList]\n",
    "    distance = preprocessDists(distance, confs)\n",
    "\n",
    "    return confs, behavior, distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2338fb",
   "metadata": {},
   "source": [
    "#### Extract confounds and behaviors of interest from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77bf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up behavioral / confound data\n",
    "conf_categories = fullData[\n",
    "    [\n",
    "        \"Acquisition\",\n",
    "        \"Gender\",\n",
    "        \"Age_in_Yrs\",\n",
    "        \"Height\",\n",
    "        \"Weight\",\n",
    "        \"BPSystolic\",\n",
    "        \"BPDiastolic\",\n",
    "        \"FS_IntraCranial_Vol\",\n",
    "        \"FS_BrainSeg_Vol\",\n",
    "        \"Larea\",\n",
    "        \"Rarea\",\n",
    "    ]\n",
    "]\n",
    "SensoryTasks = [\"ReadEng_Unadj\"]\n",
    "subjSM = fullData[SensoryTasks]\n",
    "\n",
    "full_subjList = conf_categories.merge(subjSM, on=\"Subject\").dropna().index\n",
    "full_subjList.to_series().to_csv(\"SubjectInclusionList.csv\")\n",
    "\n",
    "conf_categories = conf_categories.loc[full_subjList]\n",
    "subjSM = subjSM.loc[full_subjList]\n",
    "complete_data = fullData.loc[full_subjList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3621f9",
   "metadata": {},
   "source": [
    "### Correlation calculation for subsequent feature selection \n",
    "In this step we show an example of how correlations between the brain data (Distance or FC) are calculated within each training fold of the cross-validation folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9911d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Adjusted function to compute Spearman's correlation for a single column.\n",
    "# Now, it directly takes the column data as an argument instead of the column index and entire DataFrame.\n",
    "def corr_single_column(column_data, beh_data, behavior):\n",
    "    r, p = spearmanr(column_data, beh_data[behavior])\n",
    "    return r, p\n",
    "\n",
    "\n",
    "# Parallel version of corrDist2BEH that's more memory-efficient\n",
    "def corrDist2BEH_parallel(dist, beh_data, behavior):\n",
    "    num_processes = multiprocessing.cpu_count()  # Utilize all available cores\n",
    "\n",
    "    # Prepare column data for each task to reduce memory usage\n",
    "    # This time, instead of passing the whole DataFrame and column index,\n",
    "    # pass the data of each column directly.\n",
    "    tasks = [(dist.iloc[:, i].values, beh_data, behavior) for i in range(dist.shape[1])]\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Execute the tasks in parallel and collect the results\n",
    "        results = pool.starmap(corr_single_column, tasks)\n",
    "\n",
    "    # Separate the results into correlation coefficients and p-values\n",
    "    cr, p_vals = zip(*results)\n",
    "\n",
    "    return np.asarray(cr), np.asarray(p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c854fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### confounds are removed within each training fold on both the brain and behaviour data\n",
    "def clean_folds(cv_folds):\n",
    "    training_data = {}\n",
    "    testing_data = {}\n",
    "    for fold in cv_folds:\n",
    "        print(fold)\n",
    "\n",
    "        ### training\n",
    "        train_confounds, train_behavior, train_distance = clean_data(\n",
    "            folds[fold][\"training\"], conf_categories, subjSM, SchaeferDist\n",
    "        )\n",
    "        data_dict = {\"Dist\": train_distance, \"Beh\": train_behavior}\n",
    "        training_data[fold] = data_dict\n",
    "        ### testing\n",
    "        test_confounds, test_behavior, test_distance = clean_data(\n",
    "            folds[fold][\"testing\"], conf_categories, subjSM, SchaeferDist\n",
    "        )\n",
    "        data_dict = {\"Dist\": test_distance, \"Beh\": test_behavior}\n",
    "        testing_data[fold] = data_dict\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c7dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the cross validation folds from nb1\n",
    "f = open(\"cross_validation_folds.json\")\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "cv_folds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2164394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussianizing\n"
     ]
    }
   ],
   "source": [
    "fold_json = list(cv_folds.keys())[0]\n",
    "subjects = cv_folds[fold_json][\"training\"]\n",
    "\n",
    "### run the spearman correlation on this fold of the training data\n",
    "### save out the correlation and p-values for each fold\n",
    "corrs = np.zeros([1, 39800])\n",
    "p_vals = np.zeros([1, 39800])\n",
    "for idx, task in enumerate(subjSM.columns):\n",
    "    ### clean data for feature selection via spearman correlation\n",
    "    _, beh, dist = clean_data(subjects, conf_categories, subjSM, SchaeferDist)\n",
    "    rho, p = corrDist2BEH_parallel(dist, beh, task)\n",
    "    corrs[idx, :] = rho\n",
    "    p_vals[idx, :] = p\n",
    "corrs = pd.DataFrame(corrs)\n",
    "corrs.columns = dist.columns\n",
    "corrs.index = beh.columns\n",
    "p_vals = pd.DataFrame(p_vals)\n",
    "p_vals.columns = dist.columns\n",
    "p_vals.index = beh.columns\n",
    "out_int = fold_json.split(\"d\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7e070",
   "metadata": {},
   "source": [
    "Note this notebook simply shows the set up. \n",
    "We actually use a script to run this in parallel on the cluster \n",
    "\n",
    "scripts associated are run_dist_corr_cpm_cluster.py and slurm_cpm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b1713",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We'll use the correlations of brain data and reading now to extract our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0021d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### return a set of edges present across folds by percentage.\n",
    "### 100 would return only edges / distances present across all folds\n",
    "def edges_in_threshold(EdgesPassingByTask, tasks_passing, threshold_percentage):\n",
    "    \"\"\"\n",
    "    Find edges for each task present in at least threshold_percentage of folds.\n",
    "    :param EdgesPassingByTask: Dictionary with fold keys and task-edge lists as values.\n",
    "    :param tasks_passing: List of tasks to consider.\n",
    "    :param threshold_percentage: The minimum percentage of folds an edge must appear in to be included.\n",
    "    :return: A dictionary with tasks as keys and lists of edges meeting the threshold as values.\n",
    "    \"\"\"\n",
    "    num_folds = len(EdgesPassingByTask)\n",
    "    threshold_folds = threshold_percentage * num_folds / 100\n",
    "\n",
    "    # Initialize a dictionary to store edges for each task with their fold presence count\n",
    "    edge_presence_count = {task: {} for task in tasks_passing}\n",
    "\n",
    "    # Count the occurrence of each edge across folds for each task\n",
    "    for fold, tasks_edges in EdgesPassingByTask.items():\n",
    "        for task, edges in tasks_edges.items():\n",
    "            for edge in edges:\n",
    "                if edge not in edge_presence_count[task]:\n",
    "                    edge_presence_count[task][edge] = 1\n",
    "                else:\n",
    "                    edge_presence_count[task][edge] += 1\n",
    "\n",
    "    # Filter edges based on threshold_folds and store in a final dictionary\n",
    "    tasks_with_threshold_edges = {}\n",
    "    for task, edges_counts in edge_presence_count.items():\n",
    "        # Filter edges that meet or exceed the threshold\n",
    "        threshold_edges = [\n",
    "            edge for edge, count in edges_counts.items() if count >= threshold_folds\n",
    "        ]\n",
    "        tasks_with_threshold_edges[task] = threshold_edges\n",
    "\n",
    "    return tasks_with_threshold_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d44926",
   "metadata": {},
   "source": [
    "### Threshold by Pvals and extract edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "454db37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdData(df, thr=0.01):\n",
    "    \"\"\"a function which applies a threshold to distance behavior associations\n",
    "    default is 0.01, but passing 'fdr' will run multiple comparisons correctoin instead\n",
    "    \"\"\"\n",
    "    if thr != \"fdr\":\n",
    "        out = df < thr\n",
    "    elif thr == \"fdr\":\n",
    "        out = []\n",
    "        for task in df.index:\n",
    "            passed, _, _, _ = multipletests(df.loc[task], alpha=0.05, method=\"fdr_bh\")\n",
    "            out.append(passed)\n",
    "        out = np.asarray(out)\n",
    "        out = pd.DataFrame(out, index=df.index, columns=df.columns)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7055b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### extract all data points that pass significance threshold of 0.01\n",
    "def get_sig_corrs(pvaList):\n",
    "    dataframes = []\n",
    "    for index, p_vals in enumerate(pvaList, start=1):\n",
    "        # Read the CSV file\n",
    "        df_p = pd.read_csv(p_vals, index_col=0)\n",
    "        # Apply thresholding function\n",
    "        df_p = thresholdData(df_p, 0.01)\n",
    "        dataframes.append(df_p)\n",
    "\n",
    "    # Concatenate all dataframes horizontally\n",
    "    p_fold = pd.concat(dataframes, axis=0)\n",
    "    sig_edges = list(p_fold.columns[np.where(p_fold.sum(axis=0) == 100)[0]])\n",
    "    return sig_edges\n",
    "\n",
    "\n",
    "def consistent_signage(corrList, edgeList):\n",
    "    dataframes = []\n",
    "    for index, corrs in enumerate(corrList, start=1):\n",
    "        dataframes.append(pd.read_csv(corrs, index_col=0))\n",
    "\n",
    "    # Concatenate all dataframes horizontally\n",
    "    r_fold = pd.concat(dataframes, axis=0)\n",
    "\n",
    "    df = np.sign(r_fold[edgeList]).sum(axis=0)\n",
    "\n",
    "    pos_dists = list(df[df > 1].index)\n",
    "    neg_dists = list(df[df < 1].index)\n",
    "    return pos_dists, neg_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a17318",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebakedPvalsDist=!ls {data_dir}/distCPMfeaturesReading/*pval*csv\n",
    "prebakedCorrsDist=!ls  {data_dir}/distCPMfeaturesReading/*corr*csv\n",
    "p_passed_EdgesDist=get_sig_corrs(prebakedPvalsDist)\n",
    "positiveDistances,negativeDistances=consistent_signage(prebakedCorrsDist,p_passed_EdgesDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the outputs to a json we can use in the next notebook\n",
    "filename = \"positive_distances.json\"\n",
    "# Write the JSON data to a file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump({\"ReadEng_Unadj\": positiveDistances}, f, indent=4)\n",
    "\n",
    "filename = \"negativeDistances.json\"\n",
    "# Write the JSON data to a file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump({\"ReadEng_Unadj\": negativeDistances}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebakedPvalsFC=!ls {data_dir}/funcCPMfeaturesReading/*pval*csv\n",
    "prebakedCorrsFC=!ls {data_dir}/funcCPMfeaturesReading/*corr*csv\n",
    "p_passed_EdgesFC=get_sig_corrs(prebakedPvalsFC)\n",
    "positiveFC,negativeFC=consistent_signage(prebakedCorrsFC,p_passed_EdgesFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d948d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the outputs to a json we can use in the next notebook\n",
    "filename = \"positiveFC.json\"\n",
    "# Write the JSON data to a file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump({\"ReadEng_Unadj\": positiveFC}, f, indent=4)\n",
    "\n",
    "filename = \"negativeFC.json\"\n",
    "# Write the JSON data to a file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump({\"ReadEng_Unadj\": negativeFC}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7493",
   "metadata": {},
   "source": [
    "### Finished\n",
    "Move to notebook 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
